\documentclass{letter}
\usepackage{graphicx}
\usepackage{color}
\date{Sep 1, 2014}

\newenvironment{review}%          environment name
{\textbf{Reviewer comment:}\begin{quote}}% begin code
{\end{quote}}%  

\newcommand{\todo}[1]{%                                                                                                                            
      \color{red}\textbf{[TODO]} #1\color{black}}

\newcommand{\answer}[1]{%                                                                                                                            
      \textbf{Answer:} #1}

\usepackage{hyperref}

\newcommand{\revised}[1]{\emph{#1}\color{black}}
\newcommand{\rev}[1]{\color{blue} #1\color{black}}

\begin{document}

\begin{letter}{}

\opening{Dear Editor and Reviewers,}

We would like to thank you for your thorough review and the useful
comments and suggestions formulated about our paper ``Using Imbalance Metrics to Optimize Task Clustering in Scientific Workflow Executions'' submitted to
Future Generation Computer Systems.

The remainder of this letter contains our answers to your reviews. 

We thank you again for your valuable review, and we remain available for any further information you may need.

\vspace{0.5cm}

Sincerely yours,

\vspace{1cm}

Weiwei Chen, Rafael Ferreira da Silva, Ewa Deelman, Rizos Sakellariou

\newpage

% Reviewer 1
\textbf{Reviewer 2}


\begin{review}
The authors have followed all recommendations and the paper has improved significantly. Particularly the evaluation with a larger number of VMs has shown a behavior that was not present on the previous paper evaluation. In this revised version there is now a new situation, where only one method (HDB) presents nearly constant performance despite the number of VMs increasing from 1000 to 1800, while all other methods show a slow down in performance gain. In Fig 23, gains wrt to baseline for 20 VMs are around 15\% for the best method (HIFB) and 4\% for the worst (HC). In Fig 24, gains wrt to baseline for 200 VMs changed to an increase of around 60\% for all methods, while with 1800 VMs, the best method (now HDB) has a sustained gain around 40\%, while the other 3 (including HIFB) have dropped to around 4\%. This evidences the superiority of the proposed methods as opposed to the baseline, which was not very clear with 20 VMs. Considering that load imbalance gets more
severe as new processing units are added, this new experiment was able to show that, depending on the number of VMs, the best method, in Fig 23, changed significantly to the best method shown in the new Fig 24, for applications that are both data and CPU-intensive. Therefore, I believe these changes on the methods' performance along this new scenario (Fig. 24) will be very useful for the literature and more in sync with FGCS papers, particularly the SI on SWEET.
Therefore, as a very minor revision, I believe it is important to state some findings on large-scale experiments with applications that are both data and cpu-intensive, at the Conclusions or Compilation of Results (Section 5.5).\end{review}

\answer{We added a paragraph in Section 6 as suggested:}

\revised{``We also studied the performance gains of all the proposed horizontal methods with the increase of the number of VMs. Figure 23 shows that HIFB mostly performs better than the other methods with a small number of VMs (5$\sim$25).  However, with the increase of the scale (VM has increased from 200 to 1800) as indicated in Figure 24, HDP presents nearly constant performance improvement over the baseline (around 40\%), while all other methods including HIFB have dropped to around 4\%.  This evidences the superiority of the proposed
methods as opposed to the baseline, which becomes clearer as the number of VMs increases.''}



\end{letter}
\end{document}
