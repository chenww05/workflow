
\section{Introduction}
\label{intro}

Scientific workflows have been widely used on several science disciplines to manage the execution of large-scale computations~\cite{Deelman2002,Sakellariou2010,Lathers2006,Oinn2004,Wieczorek2005,Maechling2007}. They consist of many computational tasks with complex data dependencies between them. As scientific workflows grow in complexity and importance, the research community need a deeper and broader understanding of the characteristics, features, and behaviors of workflows to drive improvements on research areas such as resource provisioning, task scheduling, and data management. 
Workflow characterization has been broadly studied by the community~\cite{Juve2013, Calasanz2008, Tolosana2011, Callaghan2011, Gunter2011, Yildiz2009, Ramakrishnan2008, Bharathi2008, Gu2013}. For instance, 
%Many computational scientists develop and use complex, data-intensive simulations and analyses~\cite{Hey2009} that are often structured as scientific workflows, which consist of many computational tasks with complex data dependencies between them. Scientific workflows continue to gain their popularity among many science disciplines, including physics~\cite{Deelman2002}, astronomy~\cite{Sakellariou2010}, biology~\cite{Lathers2006, Oinn2004}, chemistry~\cite{Wieczorek2005}, and earthquake science~\cite{Maechling2007}. 
%Researchs~\cite{Juve2013, Calasanz2008, Tolosana2011, Callaghan2011, Gunter2011, Yildiz2009, Ramakrishnan2008, Bharathi2008, Gu2013} have been conducted to elaborate the characterization of a wide variety of scientific workflows. In characterizing the execution profile for each workflow, 
Juve et al.~\cite{Juve2013} and Callaghan et al.~\cite{Callaghan2011} recently presented metrics to characterize the execution profile for each workflow as the number of job types, and I/O, memory, and CPU requirements of each type from diverse application domains. Tolosana et al.~\cite{Tolosana2011} proposed 18 metrics to characterize workflow resilience from the perspectives of the user, workflow enactor, and resource manager. Gunter et al.~\cite{Gunter2011} capture application-level logs and resource information, normalize these to standard representations, and store them in a centralized general purpose schema.

However, there are still challenges that have not been addressed yet. First, many of these workflow analysis tools use non-quantitative tools.

By quantitative, we first mean the metrics are numerical. Yildiz~\cite{Yildiz2009} discussed the appropriateness of standard modeling notations to scientific workflow modeling and present the basic scientific workflow structures. Garijo~\cite{Garijo2013} further introduced detection of common scientific workﬂow fragments using templates and execution provenance. However, these workflow structures themselves cannot serve as a criterion to evaluate the specific performance of a workflow since they are not numerical. Second, we mean these metrics must be comparable across different platforms and workflows. For example, the average task runtime~\cite{Juve2013} and the average task failure rate~\cite{Callaghan2011} are highly related to the characteristics of execution platforms (system load and resource availability etc.) and thus they do not serve as a reliable metric to guide the design of new optimization methods or a new workflow management system. 

Next, there is a lack of utilizing structural information in these workflow analysis tools. Scientific workflows are typically a graph constructed by data dependencies between computational tasks.  A data dependency means there is a data transfer between two tasks (output data for one and input data for the other). Data dependencies between workﬂow tasks play an important role particularly with the emergence of data intensive workflows~\cite{Callaghan2011}. Existing metrics~\cite{Juve2013, Callaghan2011, Bharathi2008}  (average failure rate, average task runtime, and average resource requirement etc. ) treat workflows as no difference to workloads that have no data dependencies between them. Tolosana~\cite{Tolosana2011} has touched some basic structural information such as the average number of joins in a workflow. But such metrics cannot tell us a global picture of workflow graph such as whether this workflow is dense or sparse, whether this workflow is more sensitive to one particular part of it and whether this workflow is relatively parallel or sequential. 

Furthermore, most of these metrics ignore or underestimate the influence of system overheads that play a significant role in the workflow's runtime~\cite{Chen2011, Prodan2008, Ostberg2011}. In heterogeneous distributed systems, workflows may experience different types of overheads, which are defined as the time of performing miscellaneous work other than executing users’ computational activities. Since the causes of overheads differ, the overheads have diverse distributions and behaviors. For example, the time to run a post-script that checks the return status of a computation is usually a constant. However, queue delays incurred while tasks are waiting in a batch scheduling systems can vary widely. In our work, we add overhead as a constitution of the extended workflow graph and this approach allows to apply graph manipulations and graph-theoretic algorithms to the corresponding workflow graphs. 

Finally, the community has lacked a deep understanding of these data and how they are related to the overall performance improvement of workflows. In this paper, we propose a series of quantitative and structural metrics that are able to reflect the structure of workflow graphs. Generally speaking, we believe such metrics can (i) guide the formulation of a workflow from the perspective of designers and (ii) help workflow management systems (WMS) refine their orchestration for workflow execution (task clustering and task scheduling etc.). Specifically, in this paper we analyze three usage scenarios including workflow profiling, task clustering, and task scheduling that our metrics can be applied to as discussed below. The reason why we choose these representative areas is related to general picture of how we address issues of performance optimization in scientific workflows. 
The most straightforward approach is to invest in hardware upgrade and reduce runtime, I/O latency or network latency; for example, replacing current virtual instances with ones that have more CPU, memory or storage resources \cite{Berriman2010, Juve2012}. However, this approach results in higher IT expenses. Another solution includes varied makespan-centric, DAG scheduling algorithms and heuristics \cite{Cao2008, Dong2010, Braun2001} that have been proposed and analyzed. However, these algorithms narrow themselves to the runtime of computational or data transfer jobs. While scheduling remains an NP-hard problem, structural and overhead aware solutions can give new insight to solutions that have not been previously considered. Task clustering \cite{Chen-balanced-2013, Ferreira-granularity-2013}, job throttling \cite{Humphrey2008}, pre-staging \cite{Amer2012} and many other solutions have been proposed to reduce the impact of overheads without requiring runtime improvement of computational jobs. In this paper, we not only focus on profiling major overheads occurring in the workflow management systems and revealing their structural connections, but also propose optimization oriented metrics to improve the performance of task scheduling and task clustering. 

\textbf{Workflow Profiling} is an effective dynamic analysis approach to investigate complex applications in practice. The realistic characteristics of data-intensive workflows are critical to optimal workflow orchestration and profiling is an effective approach to investigate the behaviors of such complex applications. Traditionally, the common and straightforward approach to address issues of performance optimization is using makespan-centric, DAG scheduling algorithms~\cite{Caniou2011}. While scheduling remains an NP-hard problem, overhead analysis tools can take an important role in giving new insight to solutions that have not been previously considered and that can be further understood. Task clustering~\cite{Chen2012}, job throttling~\cite{Humphrey2008}, pre-staging~\cite{Amer2012} and many other solutions have been proposed to reduce the impact of overheads through overlapping overheads and runtimes without requiring runtime improvement of computational jobs. Instead of giving the average task runtime, in Section~\ref{sec:profiling}, we propose four metrics to reflect the projection of cumulative overheads on timeline and show how they are related to the overall performance of different workflow optimization methods.    
%The reason why we are particularly interested in reflecting the projection of overlapped overheads and runtimes is that 
%During execution on several resources, the execution time and overheads occur at the same time during execution—we name that time as overlap. We indicate how the reduction and overlap help optimize the performance. Much research is underway to address issues of performance optimization. A common and straightforward approach includes makespan-centric, DAG scheduling algorithms~\cite{Caniou2011} and heuristics that have been proposed and analyzed. However, these algorithms narrow themselves to the runtime of computational or data transfer jobs. 


\textbf{Task clustering} techniques~\cite{Ostberg2011, Chen2012, Maheshwari2012, Ferreira-granularity-2013} have been developed to group fine-grained tasks into coarse-grained tasks so that the number of computational activities is reduced and their computational granularity is increased thereby reducing the system overheads. Grouping tasks without considering these dependencies may lead to data locality problems where output data produced by parent tasks are poorly distributed~\cite{Chen-balanced-2013}. Particularly, there is a tradeoff between runtime and data dependency balancing and thus a quantitative measurement of workﬂow characteristics is required to serve as a criterion to select and balance these solutions. To achieve this goal, in Session~\ref{sec:imbalance}, we propose a series of metrics based on the concept of impact factor and dependency distance that reflect the internal structure (in terms of both runtime and dependency) of the workﬂow  and use these metrics to guide the task clustering during the runtime. 

\textbf{Task scheduling}~\cite{Caniou2011, Blythe2005, Casanova2000} has long ignored or underestimate the influence of system overheads. When executing these applications on a multi-machine distributed environment, such as the Grid or the Cloud, significant system overheads may exist~\cite{Chen2011, Prodan2008, Dong2010, Yang03, Chen2012b} and the problem of choosing robust schedules becomes more and more important. Traditionally, a carefully crafted schedule is based on deterministic or statistic estimates for the execution time of computational activities that compose a workflow. However, in such an environment, this approach may prove to be grossly inefficient~\cite{Chen2012b}, as a result of various unpredictable overheads that may occur at runtime. Thus, to mitigate the impact of uncertain overheads, it is necessary to choose a schedule that guarantees overhead robustness, that is, a schedule that is affected as little as possible by various overhead changes. In Session~\ref{sec:sensitivity}, we propose a series of structural metrics to evaluate the sensitivity of workflow performance over the overheads and further use them to guide the selection of different scheduling heuristics. 

%given a task t from a workflow w, this metric measures the relationship between the number of failures of task t and the overall number of attempts made at executing t. This could be obtained from previous executions and would provide an indicator of the frequency of failure. However, in case of abstract workflows that have their tasks mapped to resources at runtime, the frequency of failure from past executions may not accurately reflect the expected behaviour in future executions, as the reliability of resources may change during each enactment.


%Task-level QoR metrics are not appropriate for such a computation (as the number of tasks in a workflow can be high, leading to inaccurate outcomes from the statistical analysis), and the selection must be undertaken using workflow-level QoRE and/or QoRR metrics. 


%from the ParaTrac paper
% With the advance of high performance distributed computing, users are able to execute various data-intensive applications by harnessing massive computing resources [1]. Though workflow management systems have been developed to alleviate the difficulties of planning, scheduling, and executing complex workflows in distributed environments [2–5], optimal workflow management still remains a challenge because of the complexity of applications. Therefore, one of important and practical demands is to understand and characterize the data-intensive applications to help workflow management systems (WMS) refine their orchestration for workflow execution. Research has been conducted to elaborate the characterization of a wide variety of scientific workflows using synthetic approaches [6,7]. 


Together, we provide a broader overview of workflow characteristics along with the structural, quantitative and overhead aware metrics. These characterizations can be widely used by the research community to develop synthetic workflows, benchmarks, and simulations for evaluating workflow management systems. 

%The significance of this work is twofold. First, ParaTrac provides an effortless way to extract and generate informative and comprehensive workflow profiles from fine- grained profiling data, which enables users to intuitively and quantitatively analysis, debug, and refine their own workflows. Second, fine-grained profiling implies its potential support for fine-grained and realistic scheduling of workflows. Therefore, fine-grained profiling by ParaTrac suggests not only the vantage of more accurate study of workflows, but also the feasilbility of enabling more flexible workflow controls for future workflow management systems.


%Workflow analysis aims at identifying potential improvements for workflow applications by implementing analysis concerns for monitoring, measuring and controlling the behavior of workflow applications and the data used by its activities. 

%To the best of our knowledge, these problems are not tackled in unison by any existing analysis approach. To cope with these problems we propose to 

%From Gil
%2. Workflow/experiment understandability, by grouping several specific workflow templates or executions within a single abstract workflow fragment, which describes them in a more generic way. This is useful for scientists to find out the different ways of performing an abstract method.

%Good for abstract
%Design patterns have been used successfully in software development and commercial business workflows. However, in the context of scientific workflows, pattern research has not been attempted and the development of scientific workflow is based mainly on the ad-hoc copy and paste approach using previously developed workflows. Development of scientific workflow patterns will provide a formal methodology to build scientific workflow by reusing patterns that have precise semantics. Currently there is a lack of commonly agreed methodology for proposing patterns in general. Scientific workflow patterns are even harder to develop due to the data-flow oriented nature of its computation model. The simplicity of a dataflow model facilitates intuitive workflow design, analysis and optimization, however it also embeds many types of implicit dependencies such as tokens, and to- ken rate which are not present in business workflows. This increases the number of possible design patterns in contrast to business workflow that considers only activation dependencies. In this paper, we have proposed an initial set of patterns for scientific workflow models that permit data and control flow modeling. We also tested these patterns on Kepler workflow management system.

The next Section gives an overview of the related work, Section~\ref{sec:model} presents our workflow and execution environment models, Section~\ref{sec:heuristics} details our heuristics and algorithms, Section~\ref{sec:experiments} reports experiments and results, and the paper closes with a discussion and conclusions.

\section{Related Work}
\label{related}

%Workflow Profiling 
%Stampede is a workflow data model for representing the performance characteristics of distributed workflows. It can track workflow restructuring employed by workflow systems at runtime. This allows queries that reference the workflow originally defined by the user to be answered using data collected during execution. 

Some work in the literature has attempted to define and model robustness with metrics. In~\cite{Ali2004}, the authors propose a general method to define a metric for robustness. First, a performance metric is chosen. In our case, this performance metric is the overall runtime including overhead duration as we want the execution time of an application to be as stable as possible. Second, one has to identify the parameters that make the performance metric uncertain. In our case, it is the duration of the individual overheads. Third, one needs to find how a modification of these parameters changes the value of the performance metric. In our case, the answer is, as an increase of the overhead generally implies an increase of the overall runtime. 
%Lastly, one has to identify the smallest variation of a parameter that makes the performance metric exceed an acceptable bound. 
A schedule $A$ is said to be more robust than another schedule $B$ if the variation for $A$ is larger than that for $B$.
%However, estimating this variation is the most difficult part as it requires to analyze deeply the structure of the problem and its inputs.
Following this approach, Canon~\cite{Canon2008} analyzed the robustness of 20 static DAG scheduling heuristics using a metric for robustness the standard deviation of the makespan over a large number of measurement. Braun et al. \cite{Braun2001} evaluated 11 heuristics examined and for the cases studied there, the relatively simple Min-min heuristic performs well in comparison to the other techniques. 
%In comparison, we focus on varying the parameters related to overhead instead of computational tasks. 

A plethora of studies on task scheduling~\cite{Chetto1990, Dong2010, Yang03, Blythe2005} have been developed in the distributed and parallel computing domains. Many of these schedulers have been extended to consider both the computational cost and communication cost. A static or statistic estimation of communication cost or data transfer delay~\cite{Dong2010, Yang03} has been considered in the scheduling problem. In contrast, we focus on the scheduling overheads that have been ignored or underestimated for long and we demonstrate how their unique timeline patterns influence the overhead robustness. 

Workflow patterns~\cite{Yu2005, Juve2013, Liu2008} are used to capture and abstract the common structure within a workflow and they give insights on designing new workflows and optimization methods.  
Yu~\cite{Yu2005} proposed a taxonomy that characterizes and classifies various approaches for building and executing workflows on Grids. They also provided a survey of several representative Grid workflow systems developed by various projects world-wide to demonstrate the comprehensiveness of the taxonomy. Juve~\cite{Juve2013} provided a characterization of workflow from 6 scientific applications and obtained task-level performance metrics (I/O, CPU and memory consumption). They also presented an execution profile for each workflow running at a typical scale and managed by the Pegasus workflow management system~\cite{Deelman2005}. Liu~\cite{Liu2008} proposed a novel pattern based time-series forecasting strategy which ulitilizes a periodical sampling plan to build representative duration series. 
%Fix it
Compared to them, we discover a common pattern of intervals existing in system overheads while executing scientific workflows. We also leverage this knowledge to evaluate the overhead robustness of existing heuristics and develop new heuristics. 

Overhead analysis \cite{Prodan2008, Chen2011} is a topic of great interest in the grid community. Stratan \cite{Stratan2008} evaluates workflow engines including DAGMan/Condor and Karajan/Globus in a real-world grid environment. Sonmez \cite{Sonmez2006} investigated the prediction of the queue delay in grids and assessed the performance and benefit of predicting queue delays based on traces gathered from various resource and production grid environments. Prodan \cite{Prodan2008} offers a grid workflow overhead classification and a systematic measurement of overheads. Our work further investigated the major overheads and their relationship with different optimization techniques. 
In this paper, we also leverage these knowledge to enhance the existing scheduling heuristics and provide insights on designing new algorithms. 


%Many existing workflow systems provide support for capturing provenance~\cite{Gunter2011}. Profile data could be stored along with provenance to provide a more complete history of the execution of a workflow. The information could also be used to improve workflow scheduling. Many scheduling algorithms require runtime and data estimates in order to optimize workflow execution [19-22]. Profile data could be used to create a performance model of the workflow that is able to generate these estimates. Similarly, profile data could be used to guide resource provisioning algorithms that require estimates of resource usage [23-26].


%There are numerous workflow systems in use within the scientific community, for exam- ple, ASKALON [16], Kepler [3], Pegasus [12], MOTEUR [19], P-Grade [24], Taverna [28], Triana [22] and Trident [6]. Workflow standards such as the Business Process Execution Language (BPEL) [5, 29] have been applied to sci- entific problems [15]. However, there has not been much work in providing a common workflow monitoring tool 


%balancing
Task clustering~\cite{Ostberg2011, Chen2012, Maheshwari2012, Ferreira-granularity-2013} merges fined-grained tasks into coarse-grained jobs. After task clustering the number of jobs is reduced and the cumulative overhead is reduced too.
The low performance of \emph{fine-grained} tasks is a common problem in widely distributed platforms where the scheduling overhead and queuing times at resources are high, such as Grid and Cloud systems. Several works have addressed the control of task granularity of bag of tasks. For instance, Muthuvelu et al.~\cite{Muthuvelu:2005:DJG:1082290.1082297} proposed a clustering algorithm that groups bag of tasks based on their runtime---tasks are grouped up to the resource capacity. Later, they extended their work~\cite{4493929} to determine task granularity based on task file size, CPU time, and resource constraints. Recently, they proposed an online scheduling algorithm~\cite{Muthuvelu2010,Muthuvelu2013170} that groups tasks based on resource network utilization, user's budget, and application deadline. Ng et al.~\cite{keat-2006} and Ang et al.~\cite{ang-2009} introduced bandwidth in the scheduling framework to enhance the performance of task scheduling. Longer tasks are assigned to resources with better bandwidth. Liu and Liao~\cite{4958835} proposed an adaptive fine-grained job scheduling algorithm to group fine-grained tasks according to processing capacity and bandwidth of the current available resources. Although these techniques significantly reduce the impact of scheduling and queuing time overhead, they are not applicable to scientific workflows, since data dependencies are not considered.


%should fix it
%We now briefly describe the scientific workflows that we have characterized.Montage: Montage [2] was created by the NASA/IPAC Infrared Science Archive as an open source toolkit that can be used to generate custom mosaics of the sky using input images in the Flexible Image Transport System (FITS) format. During the production of the final mosaic, the geometry of the output image is calculated from the input images. The inputs are then re-projected to have the same spatial scale and rotation, the background emissions in the images are corrected to have a uniform level, and the re-projected, corrected images are co-added to form the output mosaic. The Montage application has been represented as a workflow that can be executed in Grid environments such as the TeraGrid [30].CyberShake: The CyberShake workflow [31] is used by the Southern California Earthquake Center (SCEC) [32] to characterize earthquake hazards using the Probabilistic Seismic Hazard Analysis (PSHA) technique. Given a region of interest, an MPI based finite difference simulation is performed to generate Strain Green Tensors (SGTs). From the SGT data, synthetic seismograms are calculated for each of a series of predicted ruptures. Once this is done, spectral acceleration and probabilistic hazard curves are calculated from the seismograms to characterize the seismic hazard. CyberShake workflows composed of more than 800,000 jobs have been executed using the Pegasus workflow management system on the TeraGrid [33, 34]. Broadband: Broadband is a computational platform used by the Southern California Earthquake Center [32]. The objective of Broadband is to integrate a collection of motion simulation codes and calculations to produce research results of value to earthquake engineers. These codes are composed into a workflow that simulates the impact of one or more earthquakes on one of several recording stations. Researchers can use the Broadband platform to combine low frequency (less than 1.0Hz) deterministic seismograms with high frequency (∼10Hz) stochastic seismograms and calculate various ground motion intensity measures (spectral acceleration, peak ground acceleration and peak ground velocity) for building design analysis.Epigenomics: The USC Epigenome Center [35] is currently involved in mapping the epigenetic state of human cells on a genome-wide scale. The Epigenomics workflow is essentially a data processing pipeline that uses the Pegasus workflow management system to automate the execution of the various genome sequencing operations. DNA sequence data generated by the Illumina-Solexa [36] Genetic Analyzer system is split into several chunks that can be operated on in parallel. The data in each chunk is converted into a file format that can be used by the Maq software that maps short DNA sequencing reads [37, 38]. From there, the sequences are filtered to remove noisy and contaminating segments and mapped into the correct location in a reference genome, Finally, a global map of the aligned sequences is generated and the sequence density at each position in the genome is calculated. This workflow is being used by the Epigenome Center in the processing of production DNA methylation and histone modification data.LIGO Inspiral Analysis: The Laser Interferometer Gravitational Wave Observatory (LIGO) [39, 40] is attempting to detect gravitational waves produced by various events in the universe as per Einstein’s theory of general relativity. The LIGO Inspiral Analysis workflow [41] is used to analyze the data obtained from the coalescing of compact binary systems such as binary neutron stars and black holes. The time-frequency data from each of the three LIGO detectors is split into smaller blocks for analysis. For each block, the workflow generates a subset of waveforms belonging to the parameter space and computes the matched filter output. If a true inspiral has been detected, a trigger is generated that can be checked with triggers for the other detectors. Several additional consistency tests may also be added to the workflow.SIPHT: The bioinformatics project at Harvard University is conducting a wide search for small, untranslated RNAs (sRNAs) that regulate processes such as secretion and virulence in bacteria. The sRNA Identification Protocol using High-throughput Technology (SIPHT) program [42] uses a workflow to automate the search for sRNA encoding-genes for all bacterial replicons in the National Center for Biotechnology Information (NCBI) database. The kingdom-wide prediction and annotation of sRNA encoding genes involves a variety of programs that are executed in the proper order using Condor DAGMan’s [43, 44] capabilities. These involve the prediction of Rho-independent transcriptional terminators, BLAST (Basic Local Alignment Search Tools) comparisons of the inter-genetic regions of different replicons and the annotations of any sRNAs that are found.


